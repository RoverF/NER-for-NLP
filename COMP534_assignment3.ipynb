{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i5afvUbhmGo"
      },
      "source": [
        "---\n",
        "\n",
        "# University of Liverpool\n",
        "\n",
        "## COMP534 - Applied AI\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpEqsZpRKtkE"
      },
      "source": [
        "This notebook is associated with Assignment 3. Use it to complete the assignment by following the instructions provided in each section. Each section includes a text cell outlining the requirements. For additional details, refer to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGBUHRSKvf1"
      },
      "source": [
        "Use this first cell to import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8CXq-vKp7eV"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch.utils.data as data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hglJVRRslqMn"
      },
      "source": [
        "# 1. **Data Management**\n",
        "\n",
        "\n",
        "In this part, you need to:\n",
        "\n",
        "1.  define your experimental protocol (such as k-fold, cross validation, etc);\n",
        "2.\tcreate the dataloader to load the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szQFlMDMJYEh"
      },
      "outputs": [],
      "source": [
        "def read_data(file_path):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if current_sentence:\n",
        "                    sentences.append(current_sentence)\n",
        "                    current_sentence = []\n",
        "            else:\n",
        "                parts = line.split(' ', 1)  # Split into word and tag\n",
        "                if len(parts) == 2:\n",
        "                    word, tag = parts\n",
        "                    if word not in string.punctuation:\n",
        "                        current_sentence.append((word, tag))\n",
        "        if current_sentence:\n",
        "            sentences.append(current_sentence)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    word_counts = {}\n",
        "    for sentence in sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "    word_to_ix = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for word in word_counts:\n",
        "        word_to_ix[word] = len(word_to_ix)\n",
        "    return word_to_ix\n",
        "\n",
        "\n",
        "class NERDataset(data.Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        word_indices = [self.word_to_ix.get(word, self.word_to_ix['<UNK>']) for word, _ in sentence]\n",
        "        tag_indices = [self.tag_to_ix[tag] for _, tag in sentence]\n",
        "        return torch.tensor(word_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=-100)\n",
        "    return inputs_padded, targets_padded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ExKnSiapMLm"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Read all sentences\n",
        "file_path = \"ner_dataset.txt\"\n",
        "sentences = read_data(file_path)\n",
        "\n",
        "# Split into kfold-set (80%), test (20%)\n",
        "train_sentences, test_sentences = train_test_split(sentences, test_size=0.2,\n",
        "                                                   random_state=42)\n",
        "\n",
        "\n",
        "# Create vocabs\n",
        "tag_to_ix = {'O': 0, 'I-LOC': 1, 'I-PER': 2, 'I-ORG': 3, 'I-MISC': 4}\n",
        "word_to_ix = build_vocab(train_sentences)\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = NERDataset(test_sentences, word_to_ix, tag_to_ix)\n",
        "\n",
        "# prepare our k-fold\n",
        "k = 5\n",
        "batch_size = 32\n",
        "kf = KFold(k, shuffle=True, random_state=42)\n",
        "\n",
        "# Create test dataloader\n",
        "test_dataloader = data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScTrpUW8zOp4"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. **Neural Networks**\n",
        "\n",
        "Here, you need to:\n",
        "\n",
        "1.\tcreate a Recurrent Neural Network (RNN) (such as RNN, GRU, LSTM) to tackle the problem;\n",
        "2.\tcreate a Transformer Network to tackle the problem;\n",
        "3.\tdefine the necessary components to train the networks (that is, loss function, optimizers, etc);\n",
        "4.\ttrain the models;\n",
        "5.\tfor all training procedures, separately plot the loss and accuracy with respect to the epoch/iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9IIDAjO1iw-"
      },
      "outputs": [],
      "source": [
        "# Define our BiLSTM\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    #define the parent Model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3):\n",
        "        super(BiLSTM,self).__init__()\n",
        "        # Embedding layer to convert word indices to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # BiLSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        # Fully connected layer to map to output dimensions (*2 -- bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim) \n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Define the forward pass\n",
        "    def forward(self, x):\n",
        "        # Embed the input and apply dropout\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        # Pass through the BiLSTM\n",
        "        lstm_out, _ = self.lstm(embedded) \n",
        "        # Apply dropout to the LSTM output\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # Pass through the fully connected layer to get logits\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits.view(-1,5)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEHh2w7HURHZ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "        \n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        \n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        \n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "        \n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "        \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "        \n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "    \n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "    \n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "    \n",
        "\n",
        "class TransformerNER(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, pad_idx=0):\n",
        "        super(TransformerNER, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tagset_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Create padding mask: shape [batch, 1, 1, seq_len]\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        x = self.embedding(src)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "\n",
        "        output = self.fc(x)\n",
        "        return output.view(-1,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define an accuracy function to deal with the padded values\n",
        "def masked_accuracy(y_true, y_pred):\n",
        "    # Create the mask, i.e., the values that will be ignored\n",
        "    mask = (y_true != -100)\n",
        "\n",
        "    # Compare the true values with the predicted ones\n",
        "    matches_true_pred = [a == p for a,p in zip(y_true,y_pred) if p != -100]\n",
        "\n",
        "    # Compute masked accuracy (quotient between the total matches and the total valid values, i.e., the amount of non-masked values)\n",
        "    masked_acc = sum(matches_true_pred) / np.sum(mask)\n",
        "    \n",
        "    return masked_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a training loop\n",
        "def train(dataloader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    for sentences, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(sentences)\n",
        "        loss = criterion(output, labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    acc = masked_accuracy(labels.view(-1).numpy(), output.argmax(dim=-1).cpu().numpy())\n",
        "    print(f\"TRAIN -- Epoch {epoch+1}, Loss: {loss.item():.4f}, Masked Accuracy: {acc:.4f}\")\n",
        "    return acc, loss.item()\n",
        "\n",
        "def validation(dataloader, model):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentences, labels in dataloader:\n",
        "            output = model(sentences)\n",
        "            all_preds.append(output.argmax(dim=-1).cpu().numpy())\n",
        "            all_labels.append(labels.view(-1).cpu().numpy())\n",
        "\n",
        "        all_preds = np.concatenate(all_preds, axis=0)\n",
        "        all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        acc = masked_accuracy(all_labels, all_preds)\n",
        "        print(f\"VAL -- Masked Accuracy: {acc}\")\n",
        "    return acc\n",
        "\n",
        "# Define a function to plot the loss curve\n",
        "def plot_loss(model_name, train_losses):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i, acc in enumerate(train_losses):\n",
        "        plt.plot(acc, label=f'Fold{i+1}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title(f'Training Loss for {model_name}')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Define a function to plot the accuracy curve\n",
        "def plot_acc(model_name, train_accuracies, val_accuracies):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, acc in enumerate(train_accuracies):\n",
        "        plt.plot(acc, label=f'Train acc, Fold{i+1}')\n",
        "    for i, acc in enumerate(val_accuracies):\n",
        "        plt.plot(acc, label=f'Val acc, Fold{i+1}', linestyle='dashed')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title(f'Training vs Validation Accuracy for {model_name}')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and validate models \n",
        "\n",
        "# Parameters\n",
        "vocab_size = len(word_to_ix) # vocab size from word_to_ix\n",
        "embedding_dim = len(max(sentences, key=len)) # largest dimension of word embeddings\n",
        "output_dim = len(tag_to_ix)  # number of unique tags\n",
        "\n",
        "# Hyperparam for LSTM\n",
        "hidden_dim = 128  # hidden dimension of LSTM\n",
        "\n",
        "# Hyper params for Transformer\n",
        "d_model = 128                      # smaller embedding size (128 is common)\n",
        "num_heads = 4                        # fewer heads since model is smaller\n",
        "num_layers = 2                        # 2 encoder layers is sufficient\n",
        "d_ff = 256                            # feedforward layer size\n",
        "dropout = 0.2                       # regularization to prevent overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop for training, evaluating and plotting\n",
        "def train_plots(model_name, train_data, num_epochs=25):\n",
        "    results_val_acc = []\n",
        "    results_train_acc = []\n",
        "    val_acc_hist = [] \n",
        "    train_loss_hist = [] \n",
        "    train_acc_hist = []\n",
        "\n",
        "\n",
        "    overall_best_acc = 0\n",
        "    overall_best_model = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
        "        print(f'\\n--- FOLD: {fold+1}/{k} ---')\n",
        "        \n",
        "        if model_name == 'BiLSTM':\n",
        "            model = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "            criterion = nn.CrossEntropyLoss(ignore_index = -100)  \n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "        else:\n",
        "            model = TransformerNER(vocab_size, output_dim, d_model, num_heads, num_layers, d_ff, embedding_dim, dropout)\n",
        "            criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        train_sentences = [sentences[i] for i in train_idx]\n",
        "        val_sentences = [sentences[i] for i in val_idx]\n",
        "\n",
        "        train_dataset = NERDataset(train_sentences, word_to_ix, tag_to_ix)\n",
        "        val_dataset = NERDataset(val_sentences, word_to_ix, tag_to_ix)\n",
        "\n",
        "        train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "        val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        best_model = model\n",
        "        best_acc = 0\n",
        "        best_val_acc = 0\n",
        "\n",
        "        # Implement LR Scheduler\n",
        "        scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            acc, loss = train(train_dataloader, model, criterion, optimizer,epoch)\n",
        "            train_losses.append(loss)\n",
        "            train_accuracies.append(acc)\n",
        "            v_acc = validation(val_dataloader, model)\n",
        "            val_accuracies.append(v_acc)\n",
        "            scheduler.step()\n",
        "\n",
        "            if v_acc > best_val_acc:\n",
        "                best_model = copy.deepcopy(model)\n",
        "                best_val_acc = v_acc\n",
        "            \n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "        \n",
        "        results_val_acc.append(best_val_acc)\n",
        "        results_train_acc.append(best_acc)\n",
        "    \n",
        "        if best_val_acc > overall_best_acc:\n",
        "            overall_best_model = copy.deepcopy(best_model)\n",
        "            overall_best_acc = best_val_acc\n",
        "        \n",
        "        train_acc_hist.append(train_accuracies)\n",
        "        train_loss_hist.append(train_losses)\n",
        "        val_acc_hist.append(val_accuracies)\n",
        "    \n",
        "    plot_loss(model_name, train_loss_hist)\n",
        "    plot_acc(model_name, train_acc_hist, val_acc_hist)\n",
        "\n",
        "    avg_val_acc = sum(results_val_acc)/len(results_val_acc)\n",
        "    print(f'best validation accuracy per fold {results_val_acc}')\n",
        "    print(f'avg overall validation accuracy: {avg_val_acc}')\n",
        "\n",
        "    avg_acc = sum(results_train_acc)/len(results_train_acc)\n",
        "    print(f'best train accuracy per fold {results_train_acc}')\n",
        "    print(f'avg overall train accuracy: {avg_acc}')\n",
        "    return overall_best_model, avg_val_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BiLSTM_NER, LSTM_acc = train_plots('BiLSTM', train_sentences)\n",
        "\n",
        "transformer, transformer_acc = train_plots('Transformer', train_sentences, 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RBW58of0ZDo"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. **Evaluate models**\n",
        "\n",
        "Here, you need to:\n",
        "\n",
        "1.\tevaluate the model (the best ones you obtained in the above stage).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHWwdXg32BEl"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model and generate reports\n",
        "def evaluate_model_with_metrics(model, test_loader, class_names, model_name):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    print(f\"results of our final model: {model_name}\")\n",
        "    with torch.no_grad():\n",
        "        for sentences, labels in test_loader:\n",
        "            output = model(sentences)\n",
        "\n",
        "            all_preds.append(output.argmax(dim=-1).cpu().numpy())\n",
        "            all_labels.append(labels.view(-1).cpu().numpy())\n",
        "\n",
        "        all_preds = np.concatenate(all_preds, axis=0)\n",
        "        all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        # remove padding values\n",
        "        all_preds = np.delete(all_preds, np.where(all_labels == -100))\n",
        "        all_labels = np.delete(all_labels, np.where(all_labels == -100))\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
        "    overall_accuracy = report['accuracy']\n",
        "    print(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')\n",
        "    \n",
        "    # Convert classification report to DataFrame\n",
        "    df_report = pd.DataFrame(report).transpose()\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "    \n",
        "    return df_report\n",
        "\n",
        "# Find the best model and call the function\n",
        "if LSTM_acc > transformer_acc:\n",
        "    final_model = BiLSTM_NER\n",
        "    model_name = 'BiLSTM'\n",
        "else:\n",
        "    final_model = transformer\n",
        "    model_name = 'Transformer'\n",
        "\n",
        "df_results = evaluate_model_with_metrics(final_model, test_dataloader, tag_to_ix.keys(), model_name)\n",
        "\n",
        "# Display results table\n",
        "print(df_results)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
